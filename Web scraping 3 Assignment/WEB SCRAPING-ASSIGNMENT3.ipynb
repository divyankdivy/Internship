{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c24025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product you want to search on Amazon.in: guitar\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon_products(product):\n",
    "    search_url = f\"https://www.amazon.in/s?k={product.replace(' ', '+')}\"\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    product_list = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "\n",
    "    for product in product_list:\n",
    "        title = product.find('h2').text.strip()\n",
    "        price = product.find('span', {'class': 'a-price-whole'}).text.strip()\n",
    "        rating = product.find('span', {'class': 'a-icon-alt'}).text.strip()\n",
    "        link = 'https://www.amazon.in' + product.find('a', {'class': 'a-link-normal'})['href']\n",
    "\n",
    "        print('Title:', title)\n",
    "        print('Price:', price)\n",
    "        print('Rating:', rating)\n",
    "        print('Link:', link)\n",
    "        print('--------------------------------------')\n",
    "\n",
    "# Get user input\n",
    "search_input = input(\"Enter the product you want to search on Amazon.in: \")\n",
    "\n",
    "# Call the function to search for the products\n",
    "search_amazon_products(search_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10cfb679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product you want to search on Amazon.in: guitar\n",
      "Data saved to amazon_products.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_product_details(product):\n",
    "    search_url = f\"https://www.amazon.in/s?k={product.replace(' ', '+')}\"\n",
    "    products_data = []\n",
    "\n",
    "    for page in range(1, 4):  # Scraping first 3 pages of search results\n",
    "        url = search_url + f\"&page={page}\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        product_list = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "\n",
    "        for product in product_list:\n",
    "            details = {}\n",
    "            title_elem = product.find('h2')\n",
    "            if title_elem:\n",
    "                details['Brand Name'] = title_elem.find('span', {'class': 'a-size-base-plus a-color-base a-text-normal'}).text.strip()\n",
    "                details['Name of the Product'] = title_elem.text.strip()\n",
    "            else:\n",
    "                details['Brand Name'] = '-'\n",
    "                details['Name of the Product'] = '-'\n",
    "\n",
    "            price_elem = product.find('span', {'class': 'a-price-whole'})\n",
    "            details['Price'] = price_elem.text.strip() if price_elem else '-'\n",
    "\n",
    "            return_elem = product.find('div', {'class': 'a-text-center s-align-children-center'})\n",
    "            details['Return/Exchange'] = return_elem.text.strip() if return_elem else '-'\n",
    "\n",
    "            delivery_elem = product.find('span', {'class': 's-delivery-icon-map'})\n",
    "            details['Expected Delivery'] = delivery_elem.text.strip() if delivery_elem else '-'\n",
    "\n",
    "            availability_elem = product.find('span', {'class': 'a-size-base'})\n",
    "            details['Availability'] = availability_elem.text.strip() if availability_elem else '-'\n",
    "\n",
    "            link_elem = product.find('a', {'class': 'a-link-normal'})\n",
    "            details['Product URL'] = 'https://www.amazon.in' + link_elem['href'] if link_elem else '-'\n",
    "\n",
    "            products_data.append(details)\n",
    "\n",
    "    return products_data\n",
    "\n",
    "# Get user input\n",
    "search_input = input(\"Enter the product you want to search on Amazon.in: \")\n",
    "\n",
    "# Scrape product details\n",
    "product_details = scrape_product_details(search_input)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(product_details)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv('amazon_products.csv', index=False)\n",
    "\n",
    "print(\"Data saved to amazon_products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216b9fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the keywords\n",
    "keywords = [\"fruits\", \"cars\", \"Machine Learning\", \"guitar\", \"cakes\"]\n",
    "\n",
    "# Create a folder to store the images\n",
    "if not os.path.exists(\"images\"):\n",
    "    os.mkdir(\"images\")\n",
    "\n",
    "# Loop through the keywords\n",
    "for keyword in keywords:\n",
    "\n",
    "    # Create the URL for the Google Image search\n",
    "    url = \"https://www.google.com/search?q=\" + keyword + \"&tbm=isch\"\n",
    "\n",
    "    # Make a request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the response\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the image results\n",
    "    image_results = soup.find_all(\"img\", class_=\"rg_i\")\n",
    "\n",
    "    # Loop through the image results\n",
    "    for image_result in image_results[:10]:\n",
    "\n",
    "        # Get the image URL\n",
    "        src = image_result[\"src\"]\n",
    "\n",
    "        # Download the image\n",
    "        image = requests.get(src)\n",
    "\n",
    "        # Save the image\n",
    "        with open(os.path.join(\"images\", keyword + \"_\" + str(i) + \".jpg\"), \"wb\") as f:\n",
    "            f.write(image.content)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "# Check if the images were downloaded successfully\n",
    "for keyword in keywords:\n",
    "    for i in range(10):\n",
    "        image_path = os.path.join(\"images\", keyword + \"_\" + str(i) + \".jpg\")\n",
    "        if not os.path.exists(image_path):\n",
    "            print(\"Error: Image not downloaded for keyword\", keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a2940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "def scrape_smartphone_details(search_query):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "\n",
    "    url = \"https://www.flipkart.com\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Close the login pop-up if present\n",
    "    try:\n",
    "        close_button = driver.find_element(By.CSS_SELECTOR, \"button._2KpZ6l._2doB4z\")\n",
    "        close_button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Search for the smartphone\n",
    "    search_box = driver.find_element(By.CSS_SELECTOR, \"input.LM6RPg\")\n",
    "    search_box.clear()\n",
    "    search_box.send_keys(search_query)\n",
    "    search_box.submit()\n",
    "\n",
    "    smartphone_details = []\n",
    "\n",
    "    product_containers = driver.find_elements(By.CSS_SELECTOR, \"div._1AtVbE\")\n",
    "    for container in product_containers:\n",
    "        smartphone = {}\n",
    "\n",
    "        # Scrape the details\n",
    "        smartphone['Brand Name'] = container.find_element(By.CSS_SELECTOR, \"div._2WkVRV\").text.strip()\n",
    "        smartphone['Smartphone Name'] = container.find_element(By.CSS_SELECTOR, \"aIRwRs._1u8_Aa\").text.strip()\n",
    "        smartphone['Colour'] = container.find_element(By.CSS_SELECTOR, \"div._1lRcqv span\").text.strip()\n",
    "        smartphone['RAM'] = container.find_element(By.XPATH, \".//ul/li[1]\").text.strip()\n",
    "        smartphone['Storage(ROM)'] = container.find_element(By.XPATH, \".//ul/li[2]\").text.strip()\n",
    "        smartphone['Primary Camera'] = container.find_element(By.XPATH, \".//ul/li[3]\").text.strip()\n",
    "        smartphone['Secondary Camera'] = container.find_element(By.XPATH, \".//ul/li[4]\").text.strip()\n",
    "        smartphone['Display Size'] = container.find_element(By.XPATH, \".//ul/li[5]\").text.strip()\n",
    "        smartphone['Battery Capacity'] = container.find_element(By.XPATH, \".//ul/li[6]\").text.strip()\n",
    "        smartphone['Price'] = container.find_element(By.CSS_SELECTOR, \"._30jeq3._1_WHN1\").text.strip()\n",
    "        smartphone['Product URL'] = container.find_element(By.CSS_SELECTOR, \"aIRwRs._2Q-N-k\").get_attribute('href')\n",
    "\n",
    "        # Append the smartphone details to the list\n",
    "        smartphone_details.append(smartphone)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return smartphone_details\n",
    "\n",
    "# Get user input for the smartphone search query\n",
    "search_query = input(\"Enter the smartphone name to search on Flipkart: \")\n",
    "\n",
    "# Scrape smartphone details\n",
    "smartphone_details = scrape_smartphone_details(search_query)\n",
    "\n",
    "# Convert the scraped data into a DataFrame\n",
    "df = pd.DataFrame(smartphone_details)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv('smartphone_details.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ebfd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def scrape_coordinates(city_name):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "\n",
    "    # Open Google Maps\n",
    "    driver.get(\"https://www.google.com/maps\")\n",
    "\n",
    "    # Search for the city\n",
    "    search_box = driver.find_element(By.CSS_SELECTOR,\"input.tactile-searchbox-input\")\n",
    "    search_box.send_keys(city_name)\n",
    "    search_box.submit()\n",
    "\n",
    "    # Get the coordinates from the URL\n",
    "    url = driver.current_url\n",
    "    coordinates = url.split(\"/@\")[1].split(\",\")[:2]\n",
    "\n",
    "    # Extract latitude and longitude\n",
    "    latitude = coordinates[0]\n",
    "    longitude = coordinates[1]\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return latitude, longitude\n",
    "\n",
    "# Get user input for the city name\n",
    "city_name = input(\"Enter the city name: \")\n",
    "\n",
    "# Scrape the coordinates\n",
    "latitude, longitude = scrape_coordinates(city_name)\n",
    "\n",
    "# Display the results\n",
    "print(\"Latitude:\", latitude)\n",
    "print(\"Longitude:\", longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1601a8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_laptop_details():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    laptop_details = []\n",
    "\n",
    "    laptop_containers = soup.find_all('div', class_='TopNumbeHeading sticky-footer')\n",
    "    for container in laptop_containers:\n",
    "        laptop = {}\n",
    "        laptop['Brand Name'] = container.find('div', class_='Top10-Seller-Mob-Name').text.strip()\n",
    "        laptop['Smartphone Name'] = container.find('div', class_='TopNumbeHeading sticky-footer').text.strip()\n",
    "        specs = container.find_all('div', class_='TopNumbeHeading sticky-footer')\n",
    "\n",
    "        for spec in specs:\n",
    "            key = spec.find('div', class_='Block-price').text.strip()\n",
    "            value = spec.find('div', class_='Product-specs').text.strip()\n",
    "            laptop[key] = value\n",
    "\n",
    "        laptop_details.append(laptop)\n",
    "\n",
    "    return laptop_details\n",
    "\n",
    "# Scrape laptop details\n",
    "laptop_details = scrape_laptop_details()\n",
    "\n",
    "# Convert the scraped data into a DataFrame\n",
    "df = pd.DataFrame(laptop_details)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv('gaming_laptops.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "256dbbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "def scrape_billionaires():\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "    driver.get(url)\n",
    "\n",
    "    billionaire_details = []\n",
    "\n",
    "    # Wait for the page to load\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    # Accept the cookie consent\n",
    "    try:\n",
    "        accept_button = driver.find_element(By.CSS_SELECTOR, \"button[title='Accept All Cookies']\")\n",
    "        accept_button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Scrape the billionaire details\n",
    "    rows = driver.find_elements(By.CSS_SELECTOR, \"div.table-row\")\n",
    "\n",
    "    for row in rows:\n",
    "        billionaire = {}\n",
    "        billionaire['Rank'] = row.find_element(By.CSS_SELECTOR, \"div.rank\").text.strip()\n",
    "        billionaire['Name'] = row.find_element(By.CSS_SELECTOR, \"div.personName\").text.strip()\n",
    "        billionaire['Net worth'] = row.find_element(By.CSS_SELECTOR, \"div.netWorth\").text.strip()\n",
    "        billionaire['Age'] = row.find_element(By.CSS_SELECTOR, \"div.age\").text.strip()\n",
    "        billionaire['Citizenship'] = row.find_element(By.CSS_SELECTOR, \"div.countryOfCitizenship\").text.strip()\n",
    "        billionaire['Source'] = row.find_element(By.CSS_SELECTOR, \"div.source-column\").text.strip()\n",
    "        billionaire['Industry'] = row.find_element(By.CSS_SELECTOR, \"div.category\").text.strip()\n",
    "\n",
    "        billionaire_details.append(billionaire)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return billionaire_details\n",
    "\n",
    "# Scrape billionaire details\n",
    "billionaire_details = scrape_billionaires()\n",
    "\n",
    "# Convert the scraped data into a DataFrame\n",
    "df = pd.DataFrame(billionaire_details)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv('billionaire_details.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee1b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Function to scroll the page and load more comments\n",
    "def scroll_to_load_comments(driver):\n",
    "    # Scroll to the bottom of the page\n",
    "    driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    time.sleep(3)  # Wait for the comments to load\n",
    "\n",
    "# Set up the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "# Open the YouTube video\n",
    "video_url = \"https://www.youtube.com/watch?v=2Vv-BfVoq4g\"\n",
    "driver.get(video_url)\n",
    "\n",
    "# Scroll to load more comments\n",
    "while True:\n",
    "    scroll_to_load_comments(driver)\n",
    "\n",
    "    # Check if at least 500 comments have been loaded\n",
    "    comment_count = len(driver.find_elements(By.CSS_SELECTOR, \"#comment #content-text\"))\n",
    "    if comment_count >= 500:\n",
    "        break\n",
    "\n",
    "# Extract the comments, comment upvotes, and time posted\n",
    "comments = driver.find_elements(By.CSS_SELECTOR, \"#comment #content-text\")\n",
    "upvotes = driver.find_elements(By.CSS_SELECTOR, \"#comment #vote-count-middle\")\n",
    "time_posts = driver.find_elements(By.CSS_SELECTOR, \"#comment-header #published-time-text\")\n",
    "\n",
    "# Store the extracted data in lists\n",
    "comment_list = [comment.text for comment in comments]\n",
    "upvote_list = [upvote.text for upvote in upvotes]\n",
    "time_list = [time_post.get_attribute(\"title\") for time_post in time_posts]\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame with the extracted data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Comment\": comment_list,\n",
    "    \"Upvotes\": upvote_list,\n",
    "    \"Time\": time_list\n",
    "})\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv(\"youtube_comments.csv\", index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91550533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Hostel Name, Distance, Rating, Total Reviews, Overall Reviews, Privates from Price, Dorms from Price, Facilities, Description]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL for London hostels\n",
    "url = \"https://www.hostelworld.com/search?city=London&country=England\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all hostel containers\n",
    "hostel_containers = soup.find_all(\"div\", class_=\"fabresult\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "hostel_names = []\n",
    "distances = []\n",
    "ratings = []\n",
    "total_reviews = []\n",
    "overall_reviews = []\n",
    "privates_prices = []\n",
    "dorms_prices = []\n",
    "facilities = []\n",
    "descriptions = []\n",
    "\n",
    "# Extract data from each hostel container\n",
    "for container in hostel_containers:\n",
    "    # Hostel name\n",
    "    hostel_name = container.find(\"h2\", class_=\"title\").text.strip()\n",
    "    hostel_names.append(hostel_name)\n",
    "\n",
    "    # Distance from city centre\n",
    "    distance = container.find(\"span\", class_=\"distance\").text.strip()\n",
    "    distances.append(distance)\n",
    "\n",
    "    # Rating\n",
    "    rating = container.find(\"div\", class_=\"score orange\").text.strip()\n",
    "    ratings.append(rating)\n",
    "\n",
    "    # Total reviews\n",
    "    total_review = container.find(\"div\", class_=\"reviews\").text.strip()\n",
    "    total_reviews.append(total_review)\n",
    "\n",
    "    # Overall reviews\n",
    "    overall_review = container.find(\"div\", class_=\"keyword\").text.strip()\n",
    "    overall_reviews.append(overall_review)\n",
    "\n",
    "    # Privates from price\n",
    "    privates_price = container.find(\"div\", class_=\"price-col\").find(\"a\").text.strip()\n",
    "    privates_prices.append(privates_price)\n",
    "\n",
    "    # Dorms from price\n",
    "    dorms_price = container.find(\"div\", class_=\"price-col\").find_all(\"a\")[1].text.strip()\n",
    "    dorms_prices.append(dorms_price)\n",
    "\n",
    "    # Facilities\n",
    "    facility_tags = container.find_all(\"div\", class_=\"facilities\")\n",
    "    facility_list = [tag.text.strip() for tag in facility_tags]\n",
    "    facilities.append(\", \".join(facility_list))\n",
    "\n",
    "    # Property description\n",
    "    description = container.find(\"div\", class_=\"ratings\").find_next_sibling(\"div\").text.strip()\n",
    "    descriptions.append(description)\n",
    "\n",
    "# Create a DataFrame with the extracted data\n",
    "data = {\n",
    "    \"Hostel Name\": hostel_names,\n",
    "    \"Distance\": distances,\n",
    "    \"Rating\": ratings,\n",
    "    \"Total Reviews\": total_reviews,\n",
    "    \"Overall Reviews\": overall_reviews,\n",
    "    \"Privates from Price\": privates_prices,\n",
    "    \"Dorms from Price\": dorms_prices,\n",
    "    \"Facilities\": facilities,\n",
    "    \"Description\": descriptions\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv(\"hostels_london.csv\", index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de7dd10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
